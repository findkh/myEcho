import streamlit as st
from langchain_community.llms import Ollama
from utils import print_messages
from langchain_core.messages import ChatMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.output_parsers import StrOutputParser

# Ollama ëª¨ë¸ ì´ˆê¸°í™”
llm = Ollama(model="EEVE-Korean-Instruct-10.8B")

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="MyEcho", page_icon="ğŸ¦¦")
st.title("Chat MyEcho ğŸ¦¦")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'messages' not in st.session_state:
    st.session_state['messages'] = []
if 'chat_history' not in st.session_state:
    st.session_state['chat_history'] = ChatMessageHistory()

# ì±„íŒ… ë©”ì‹œì§€ ì¶œë ¥
print_messages()

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
user_input = st.chat_input("Watermoonë‹˜ì— ëŒ€í•œ ê¶ê¸ˆí•œ ì‚¬í•­ì„ ë¬¼ì–´ë³´ì„¸ìš”! ğŸ‘€")

if user_input:
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ ë° íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
    st.session_state['messages'].append(ChatMessage(role="user", content=user_input))
    st.session_state['chat_history'].add_user_message(user_input)

    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶œë ¥
    st.chat_message("user").write(user_input)

    # í”„ë¡¬í”„íŠ¸ë¥¼ ì„¤ì •í•˜ì—¬ Watermoonì— ëŒ€í•œ ë‹µë³€ ìƒì„±
    prompt = ChatPromptTemplate.from_template("""
                ë‚˜ëŠ” 'MyEcho'ë¡œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
                ì‚¬ìš©ìê°€ ì–´ë–¤ ì§ˆë¬¸ì„ í•˜ë“ , ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•´ 'Watermoon'ì´ë¼ëŠ” ì‚¬ëŒì— ê´€í•œ ë‹µë³€ì„ ê°„ëµí•˜ê²Œ ì œê³µí•©ë‹ˆë‹¤. 
                ì§ˆë¬¸: {query}\n ë‹µë³€:""")

    # ì‘ë‹µ ìƒì„± ì¤‘ ìƒíƒœ í‘œì‹œ
    response_placeholder = st.empty()  # ì‘ë‹µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•  ë¹ˆ ê³µê°„
    with st.spinner("ë‹µë³€ ìƒì„±ì¤‘..."):
        # ì´ì „ ëŒ€í™” ë‚´ìš© ì¶”ê°€
        messages = st.session_state['chat_history'].messages
        
        # ìµœê·¼ 5ê°œì˜ ì§ˆë¬¸ê³¼ ë‹µë³€ ì„¸íŠ¸ë§Œ ì‚¬ìš©
        recent_messages = messages[-10:]  # 5ê°œì˜ ì§ˆë¬¸ê³¼ ë‹µë³€ ì„¸íŠ¸ëŠ” ì´ 10ê°œì˜ ë©”ì‹œì§€
        
        # ë©”ì‹œì§€ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        previous_messages = ""
        for i in range(0, len(recent_messages), 2):
            if i + 1 < len(recent_messages):
                previous_messages += f"human: {recent_messages[i].content}\n"
                previous_messages += f"ai: {recent_messages[i + 1].content}\n"
        
        final_prompt = f"{previous_messages}\n\n{prompt.format(query=user_input)}"

        response = ""
        # LLMì˜ ìŠ¤íŠ¸ë¦¬ë° ê¸°ëŠ¥ ì‚¬ìš©
        for chunk in llm.stream(final_prompt):
            response += chunk
            # ë¹ˆ ê³µê°„ì— ì‹¤ì‹œê°„ìœ¼ë¡œ ì‘ë‹µ ì—…ë°ì´íŠ¸
            response_placeholder.write(response)
    
    # ìµœì¢… ì‘ë‹µ ì €ì¥ ë° í™”ë©´ì— ì¶œë ¥
    st.session_state['messages'].append(ChatMessage(role="assistant", content=response))
    st.session_state['chat_history'].add_ai_message(response)
    response_placeholder.empty()  # ë¹ˆ ê³µê°„ì„ ë¹„ìš°ê³  ìµœì¢… ì‘ë‹µì„ ì¶œë ¥
    st.chat_message("assistant").write(response)