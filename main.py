import streamlit as st
from langchain_community.llms import Ollama
from langchain_core.messages import ChatMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.chat_message_histories import ChatMessageHistory
from sentence_transformers import SentenceTransformer
import pandas as pd
import chromadb
from chromadb.config import Settings
import os
from tqdm import tqdm

# ChromaDB ì„¤ì •
settings = Settings(
    persist_directory="./chromadb_data"
)
client = chromadb.PersistentClient(settings=settings)

# SentenceTransformer ëª¨ë¸ ì´ˆê¸°í™”
model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')

def embedding_exists(directory):
    """ì„ë² ë”© ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜."""
    return os.path.exists(directory) and len(os.listdir(directory)) > 0

def insert_data_into_collection(df, model, collection):
    """ë°ì´í„°í”„ë ˆì„ì„ ë°›ì•„ ì„ë² ë”©ì„ ìƒì„±í•˜ê³  ChromaDB ì»¬ë ‰ì…˜ì— ì‚½ì…í•˜ëŠ” í•¨ìˆ˜."""
    print("insert_data_into_collection í˜¸ì¶œë¨")
    ids, metadatas, embeddings = [], [], []

    # ë°ì´í„°í”„ë ˆì„ì˜ ê° í–‰ì— ëŒ€í•´ ì„ë² ë”© ìƒì„±
    for index, row in tqdm(df.iterrows(), total=df.shape[0]):
        text = row['text']
        metadata = {"text": text}
        embedding = model.encode(text, normalize_embeddings=True)
        ids.append(str(index))
        metadatas.append(metadata)
        embeddings.append(embedding.tolist())  # numpy ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

    # ì²­í¬ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ì»¬ë ‰ì…˜ì— ì¶”ê°€
    chunk_size = 1024
    for start_idx in tqdm(range(0, len(embeddings), chunk_size)):
        end_idx = min(start_idx + chunk_size, len(embeddings))
        collection.add(
            embeddings=embeddings[start_idx:end_idx],
            ids=ids[start_idx:end_idx],
            metadatas=metadatas[start_idx:end_idx]
        )

def create_embeddings(df, model, answers):
    """ë°ì´í„°í”„ë ˆì„ì„ ë°›ì•„ ì„ë² ë”©ì„ ìƒì„±í•˜ê³  ChromaDB ì»¬ë ‰ì…˜ì— ì‚½ì…í•˜ëŠ” í•¨ìˆ˜."""
    print("create_embeddings í˜¸ì¶œë¨")
    ids, metadatas, embeddings = [], [], []

    for index, row in tqdm(df.iterrows(), total=df.shape[0]):
        text = row['text']
        metadata = {"text": text}
        embedding = model.encode(text, normalize_embeddings=True)
        ids.append(str(index))
        metadatas.append(metadata)
        embeddings.append(embedding.tolist())  # numpy ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

    # ì²­í¬ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ì»¬ë ‰ì…˜ì— ì¶”ê°€
    chunk_size = 1024
    for start_idx in tqdm(range(0, len(embeddings), chunk_size)):
        end_idx = min(start_idx + chunk_size, len(embeddings))
        answers.add(
            embeddings=embeddings[start_idx:end_idx],
            ids=ids[start_idx:end_idx],
            metadatas=metadatas[start_idx:end_idx]
        )
    answers.persist()  # ë°ì´í„° ì €ì¥

# ChromaDB ì»¬ë ‰ì…˜ ì„¤ì •
collection_name = "test"
collections = client.list_collections()
collection_exists = any(collection.name == collection_name for collection in collections)

if collection_exists:
    print("ì»¬ë ‰ì…˜ ì¡´ì¬")
    answers = client.get_collection(name=collection_name)
else:
    print("ì»¬ë ‰ì…˜ ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
    answers = client.create_collection(name=collection_name)
    df = pd.read_csv("./watermoonInfo.csv")
    insert_data_into_collection(df, model, answers)

# Ollama ëª¨ë¸ ì´ˆê¸°í™”
llm = Ollama(model="EEVE-Korean-Instruct-10.8B")

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="MyEcho", page_icon="ğŸ¦¦")
st.title("Chat MyEcho ğŸ¦¦")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'messages' not in st.session_state:
    st.session_state['messages'] = []
if 'chat_history' not in st.session_state:
    st.session_state['chat_history'] = ChatMessageHistory()

# ì±„íŒ… ë©”ì‹œì§€ ì¶œë ¥ í•¨ìˆ˜
def print_messages():
    """ì„¸ì…˜ ìƒíƒœì˜ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜."""
    for message in st.session_state['messages']:
        st.chat_message(message.role).write(message.content)

print_messages()

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
user_input = st.chat_input("Watermoonë‹˜ì— ëŒ€í•œ ê¶ê¸ˆí•œ ì‚¬í•­ì„ ë¬¼ì–´ë³´ì„¸ìš”! ğŸ‘€")

if user_input:
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ ë° íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
    st.session_state['messages'].append(ChatMessage(role="user", content=user_input))
    st.session_state['chat_history'].add_user_message(user_input)
    st.chat_message("user").write(user_input)

    # í”„ë¡¬í”„íŠ¸ ì„¤ì •
    prompt = ChatPromptTemplate.from_template("""
                ë‚˜ì˜ ì´ë¦„ì€ 'MyEcho'ë¡œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
                ì‚¬ìš©ìê°€ ì–´ë–¤ ì§ˆë¬¸ì„ í•˜ë“ , ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•´ 'Watermoon'ì´ë¼ëŠ” ì‚¬ëŒì— ê´€í•œ ë‹µë³€ì„ ê°„ëµí•˜ê²Œ ì œê³µí•©ë‹ˆë‹¤. 
                ì‚¬ìš©ìê°€ ë¬¼ì–´ë³¸ ì§ˆë¬¸: {query}
                ì•„ë˜ì˜ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ì •ë³´ë¥¼ ì„ íƒí•˜ì—¬ ë‹µë³€ ì‘ì„±í•´ì£¼ì„¸ìš”:
                {info}
                ë‹µë³€:""")
    
    # ì‘ë‹µ ìƒì„± ì¤‘ ìƒíƒœ í‘œì‹œ
    response_placeholder = st.empty()
    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        messages = st.session_state['chat_history'].messages
        recent_messages = messages[-10:]

        # ë©”ì‹œì§€ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        previous_messages = "\n".join(
            f"human: {msg.content}\n" for msg in recent_messages[::2]
        ) + "\n" + "\n".join(
            f"ai: {msg.content}\n" for msg in recent_messages[1::2]
        )

        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = model.encode(user_input, normalize_embeddings=True).tolist()
        result = answers.query(
            query_embeddings=[query_embedding],
            n_results=2
        )

        # ì •ë³´ ì¶”ì¶œ
        metadatas = result['metadatas'][0]
        info = "\n".join(str(data) for data in metadatas)

        final_prompt = f"{previous_messages}\n\n{prompt.format(query=user_input, info=info)}"

        # LLMì˜ ìŠ¤íŠ¸ë¦¬ë° ê¸°ëŠ¥ ì‚¬ìš©
        response = ""
        for chunk in llm.stream(final_prompt):
            response += chunk
            response_placeholder.write(response)

    # ìµœì¢… ì‘ë‹µ ì €ì¥ ë° í™”ë©´ì— ì¶œë ¥
    st.session_state['messages'].append(ChatMessage(role="assistant", content=response))
    st.session_state['chat_history'].add_ai_message(response)
    response_placeholder.empty()
    st.chat_message("assistant").write(response)