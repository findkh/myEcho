import os
import streamlit as st
import ollama
import pandas as pd
from langchain_core.messages import ChatMessage
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma

# ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” í•¨ìˆ˜
def initialize_vectorstore(persist_directory, embedding_model):
    if os.path.exists(persist_directory) and os.listdir(persist_directory):
        return Chroma(persist_directory=persist_directory, embedding_function=embedding_model)
    # CSV íŒŒì¼ì—ì„œ ë°ì´í„° ì½ê¸°
    df = pd.read_csv("./watermoonInfo.csv")
    texts = df['text'].tolist()
    metadatas = [{"text": text} for text in texts]
    return Chroma.from_texts(texts, embedding_model, metadatas=metadatas, persist_directory=persist_directory)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” í•¨ìˆ˜
def initialize_session_state():
    if 'messages' not in st.session_state:
        st.session_state['messages'] = []

# ë©”ì‹œì§€ ì €ì¥ í•¨ìˆ˜
def save_message(role, content):
    message = ChatMessage(role=role, content=content)
    st.session_state['messages'].append(message)

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„± í•¨ìˆ˜
def create_prompt_template(user_input, info):
    all_messages = st.session_state['messages']
    
    # ì „ì²´ ë©”ì‹œì§€ë¥¼ ì—­ìˆœìœ¼ë¡œ ì •ë ¬
    reversed_messages = list(reversed(all_messages))
    
    # ìµœì‹  ì§ˆë¬¸ì„ ì œì™¸í•œ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    if reversed_messages and reversed_messages[0].role == "user":
        reversed_messages = reversed_messages[1:]
    
    # ìµœì‹  6ê°œì˜ ëŒ€í™” ìŒë§Œ í¬í•¨
    history = reversed_messages[:6]
    
    # ìµœê·¼ ëŒ€í™” ë‚´ìš© í¬ë§·íŒ…
    recent_message_pairs = "\n".join(f"{msg.role}: {msg.content}" for msg in reversed(history))

    return f"""
        ë‚˜ì˜ ì´ë¦„ì€ 'MyEcho'ë¡œ, AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
        ì €ëŠ” ì˜¤ì§ 'watermoon'ì´ë¼ëŠ” ì‚¬ëŒì— ëŒ€í•œ ì •ë³´ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤. 
        'watermoon'ì€ ì´ í”„ë¡œê·¸ë¨ì„ ë§Œë“  ì‹¤ì¡´ ì¸ë¬¼ì…ë‹ˆë‹¤.
        
        ì•„ë˜ì— ì œê³µëœ ì •ë³´ë§Œì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. ì£¼ì–´ì§„ ì •ë³´ ì™¸ì˜ ë‚´ìš©ì„ ì¶”ì¸¡í•˜ê±°ë‚˜ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”:
        {info}
        
        ìµœê·¼ ëŒ€í™” ë‚´ìš©:
        {recent_message_pairs}
        
        ì‚¬ìš©ìê°€ ë¬¼ì–´ë³¸ ì§ˆë¬¸: {user_input}

        ë‹µë³€:
    """

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ í•¨ìˆ˜
def process_input(user_input, retriever):
    if any(keyword in user_input for keyword in ["ìš”ì•½", "ì•ˆë…•", "í•˜ì´"]):
        info = ""  # ë²¡í„° DBë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ë¹ˆê°’ìœ¼ë¡œ ì„¤ì •
        return info
    else:
        # retrieverë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        result = retriever.get_relevant_documents(user_input)

        # ê°€ì ¸ì˜¨ ë¬¸ì„œì˜ page_contentë¥¼ ì§ì ‘ ë°˜í™˜
        return result[0].page_content

# ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ í•¨ìˆ˜
def display_chat_history():
    for message in st.session_state['messages']:
        st.chat_message(message.role).write(message.content)

# LLM ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í•¨ìˆ˜
def llm_stream(model_name, prompt):
    response = ollama.chat(model_name, [{"content": prompt, "role": "user"}], stream=True)
    for chunk in response:
        yield chunk['message']['content']

# ëª¨ë¸ê³¼ Embedding ì´ˆê¸°í™”
embedding_model = HuggingFaceEmbeddings(model_name='snunlp/KR-SBERT-V40K-klueNLI-augSTS')
persist_directory = "./chromadb_data"
vectorstore = initialize_vectorstore(persist_directory, embedding_model)

# Retriever ì„¤ì •
retriever = vectorstore.as_retriever(search_kwargs={'k': 1})

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="MyEcho", page_icon="ğŸ¦¦")
st.title("Chat MyEcho ğŸ¦¦")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
initialize_session_state()

# ì‚¬ì´ë“œë°”ì—ì„œ ëŒ€í™” ì œëª© ì…ë ¥
st.sidebar.header("Chat Settings")
chat_title = st.sidebar.text_input("Enter chat title", value="My Chat")

# ì‚¬ì´ë“œë°”ì—ì„œ ëª¨ë¸ ì„ íƒ
try:
    OLLAMA_MODELS = ollama.list()["models"]
except Exception as e:
    st.warning("Ollamaë¥¼ ë¨¼ì € ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    st.stop()

model_names = [model["name"] for model in OLLAMA_MODELS]
llm_name = st.sidebar.selectbox("Choose Agent", [""] + model_names)
if not llm_name:
    st.stop()

# ì±„íŒ… ë©”ì‹œì§€ ì¶œë ¥
display_chat_history()

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
user_input = st.chat_input("Watermoonë‹˜ì— ëŒ€í•œ ê¶ê¸ˆí•œ ì‚¬í•­ì„ ë¬¼ì–´ë³´ì„¸ìš”! ğŸ‘€")

if user_input:
    st.chat_message("user").write(user_input)
    save_message("user", user_input)
    info = process_input(user_input, retriever)
    prompt = create_prompt_template(user_input, info)

    # AI ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°í•˜ë©´ì„œ UIì— í‘œì‹œ
    with st.chat_message("assistant"):
        chat_box = st.empty()
        response_message = chat_box.write_stream(llm_stream(llm_name, prompt))

    # ìµœì¢… ì‘ë‹µì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
    save_message("assistant", response_message)

# ì‚¬ì´ë“œë°”ì—ì„œ ëŒ€í™” ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
if st.session_state['messages']:
    chat_history_text = "\n".join(f"{msg.role}: {msg.content}" for msg in st.session_state['messages'])
    st.sidebar.download_button(
        label="Download Chat",
        data=chat_history_text,
        file_name=f"{chat_title}.txt",
        mime="text/plain"
    )

# ëŒ€í™” ë¦¬ì…‹ ë²„íŠ¼ êµ¬í˜„
if st.session_state['messages'] and st.sidebar.button("Reset Chat"):
    st.session_state.clear()
    st.rerun()
